ë„¤, ê·¸ë™ì•ˆ ë…¼ì˜ëœ ëª¨ë“  ë‚´ìš©(ì´ˆê¸° ì„¤ê³„ë¶€í„° Claude ê²€í† ì‚¬í•­, pgvector ë„ì…, LLM ì „ëµ ë³€ê²½, ë”¥ ë¦¬ì„œì¹˜ íŒŒì´í”„ë¼ì¸ ë“±)ì„ ë¹ ì§ì—†ì´ í†µí•©í•˜ì—¬ ìµœì¢… ì„¤ê³„ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ëŒ€ë¡œ ë¶„í• í•˜ì—¬ ì‘ì„±í•˜ì˜¤ë‹ˆ, ì™„ë£Œë  ë•Œê¹Œì§€ `continue`ë¥¼ ì…ë ¥í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

---

# ğŸ“Š íˆ¬ì ì¸í…”ë¦¬ì „ìŠ¤ ì‹œìŠ¤í…œ: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ ë¶„ì„ ì„¤ê³„ì„œ (Final Ver.)

> **ë¬¸ì„œ ëª©ì **: ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸(PDF)ì—ì„œ í•µì‹¬ íˆ¬ì ì‹ í˜¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë ¥ê³¼ ì„±ê³¼ë¥¼ ì¶”ì í•˜ì—¬ ì‹œì¥ì˜ ì™œê³¡ëœ ì‹ í˜¸ë¥¼ êµì •í•˜ë©°, ë‚˜ì•„ê°€ ì—­ì‚¬ì  íŒ¨í„´ ë¶„ì„ì„ í†µí•œ ì‹¬ì¸µ íˆ¬ì ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œ êµ¬ì¶•.

---

## 1. ì„¤ê³„ ê°œìš” ë° í•µì‹¬ ì›ì¹™

ë³¸ ì‹œìŠ¤í…œì€ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ìš”ì•½ì„ ë„˜ì–´, ë°ì´í„°ì˜ ì‹ ë¢°ì„±ì„ ê²€ì¦í•˜ê³  ì‹œì¥ì˜ ìˆ¨ì€ ì‹ í˜¸ë¥¼ í¬ì°©í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

### 1.1 í•µì‹¬ ì„¤ê³„ ì›ì¹™
1.  **ì‹ í˜¸ í•´ì„ ì¤‘ì‹¬ (Signal over Opinion)**: í•œêµ­ ì‹œì¥ì˜ êµ¬ì¡°ì  íŠ¹ì„±(ë§¤ìˆ˜ ì˜ê²¬ ê³¼ì‰)ì„ ê³ ë ¤í•˜ì—¬, 'ëª©í‘œê°€ í•˜í–¥ ì¡°ì •'ê³¼ ê°™ì€ ì‹¤ì§ˆì  ì‹ í˜¸ë¥¼ í¬ì°©í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
2.  **ë°ì´í„° ì •í•©ì„± (Data Integrity)**: ì‹œê°„ ì—­ìˆœ ìˆ˜ì§‘, ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ì§, í…ìŠ¤íŠ¸ í’ˆì§ˆ ì €í•˜ ë“± ì‹¤ì œ ë°ì´í„° í™˜ê²½ì˜ ë¬¸ì œë¥¼ ê¸°ìˆ ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤.
3.  **ìš´ì˜ íš¨ìœ¨ì„± (Cost Efficiency)**: LLM ëª¨ë¸ ë¼ìš°íŒ… ì „ëµì„ í†µí•´ ë¹„ìš©ì„ ìµœì í™”í•˜ê³ , ìŠ¤í† ë¦¬ì§€ ë¶„ë¦¬ë¥¼ í†µí•´ DB ë¶€í•˜ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
4.  **ì‹¬ì¸µ ë¶„ì„ (Deep Research)**: pgvectorë¥¼ í™œìš©í•œ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ìœ¼ë¡œ ê³¼ê±° íŒ¨í„´(ì˜ˆ: ë¹„ê´€ì˜ ë)ê³¼ í˜„ì¬ ìƒí™©ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

### 1.2 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ (System Architecture)

í™•ì¥ì„±ì„ ê³ ë ¤í•œ ëª¨ë“ˆí˜• êµ¬ì¡°ë¡œ, ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€ ë° ë¶„ì„ ì—”ì§„ ê³ ë„í™”ê°€ ìš©ì´í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

```mermaid
flowchart TD
    subgraph Sources ["ë°ì´í„° ì†ŒìŠ¤ (Extensible)"]
        A1["Naver Finance (êµ­ë‚´)"]
        A2["Telegram Channel (ì™¸êµ­ê³„)"]
        A3["Stock Price API (ì£¼ê°€ ë°ì´í„°)"]
    end

    subgraph Ingestion ["ìˆ˜ì§‘ ë ˆì´ì–´"]
        B1["PDF Downloader"]
        B2["Stock Price Collector"]
        B3["Deduplication (Hash & ID Check)"]
    end

    subgraph Processing ["ì²˜ë¦¬ ë ˆì´ì–´ (PDF Pipeline)"]
        C1["Text Extractor (pdfplumber)"]
        C2{"Quality Check"}
        C3["PaddleOCR Fallback"]
        C4["Summary Page Detector (Intelligent)"]
        C5["Text Chunker"]
    end

    subgraph Intelligence ["ì§€ëŠ¥ ë ˆì´ì–´ (LLM & Analytics)"]
        D1["Analyst Identifier (Time-Resistant)"]
        D2["Target Price Resolver"]
        D3["LLM Router"]
        D3_1["Gemini Flash\n(êµ¬ì¡°í™” ì¶”ì¶œ)"]
        D3_2["Claude Sonnet\n(ì‹¬ì¸µ ë¶„ì„)"]
        D4["Embedding Generator\n(text-embedding-3-small)"]
        D5["Sentiment Aggregator"]
    end

    subgraph DeepResearch ["ë”¥ ë¦¬ì„œì¹˜ ì—”ì§„"]
        E1["Pattern Matcher (pgvector)"]
        E2["Insight Generator"]
    end

    subgraph Storage ["ì €ì¥ì†Œ"]
        F1[("PostgreSQL\n(Metadata + pgvector)")]
        F2[("Object Storage\n(PDF & Raw Text)")]
    end

    subgraph Serving ["ì„œë¹™ ë ˆì´ì–´"]
        G1["Signal Dashboard"]
        G2["Analyst Tracking"]
        G3["Deep Research Report"]
    end

    A1 & A2 --> B1 --> B3 --> C1 --> C2
    C2 -- "Low Quality" --> C3 --> C4
    C2 -- "OK" --> C4 --> C5
    
    A3 --> B2 --> F1
    
    C5 --> D1 & D2 --> D3
    D3 --> D3_1 --> F1
    F1 --> D4 --> F1
    D3_1 --> D5 --> F1
    
    F1 --> E1 --> E2 --> D3_2 --> G3
    
    B1 --> F2
```

---

## 2. PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Advanced Pipeline)

ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì˜ ë‹¤ì–‘í•œ í˜•ì‹ì„ ëŒ€ì‘í•˜ê³  ë°ì´í„° ì¶”ì¶œ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ê³¼ì •ì…ë‹ˆë‹¤.

### 2.1 í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í’ˆì§ˆ ê²€ì¦ (Quality Assurance)
ì¼ë¶€ PDFëŠ” ì´ë¯¸ì§€ ìœ„ì— **ê°€ì§œ í…ìŠ¤íŠ¸ ë ˆì´ì–´**(ë³µì‚¬ ë°©ì§€ ëª©ì )ê°€ ì¡´ì¬í•˜ì—¬ ì¶”ì¶œ ì‹œ ê¹¨ì§„ í…ìŠ¤íŠ¸ê°€ ë°˜í™˜ë©ë‹ˆë‹¤. ì´ë¥¼ ê°ì§€í•˜ì—¬ OCRë¡œ ìë™ ì „í™˜í•˜ëŠ” ë¡œì§ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

```python
class TextQualityChecker:
    GARBAGE_THRESHOLD = 0.3  # ê¹¨ì§„ ë¬¸ì í—ˆìš© ë¹„ìœ¨
    
    def assess(self, text: str) -> tuple[bool, float]:
        if len(text) < 100: 
            return False, 0.0
        
        # ìœ íš¨ ë¬¸ì(í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê¸°í˜¸) ë¹„ìœ¨ ê³„ì‚°
        valid_chars = sum(1 for c in text if self._is_valid(c))
        quality_score = valid_chars / len(text)
        
        # ìœ íš¨ ë¬¸ì ë¹„ìœ¨ì´ ì„ê³„ê°’ ë¯¸ë§Œì´ë©´ ë¶ˆëŸ‰ íŒì • -> OCR í´ë°± í•„ìš”
        return quality_score >= (1 - self.GARBAGE_THRESHOLD), quality_score

    def _is_valid(self, char: str) -> bool:
        cp = ord(char)
        return (
            0xAC00 <= cp <= 0xD7A3 or  # í•œê¸€ ì™„ì„±í˜•
            0x0041 <= cp <= 0x007A or  # ì˜ë¬¸
            0x0030 <= cp <= 0x0039 or  # ìˆ«ì
            char in ' .,()%\n\t-+'     # ì¼ë°˜ ê¸°í˜¸
        )

# íŒŒì´í”„ë¼ì¸ ì ìš© ì˜ˆì‹œ
def extract_text(pdf_path: str) -> tuple[str, str]:
    # 1ì°¨: pdfplumber ì‹œë„ (í‘œ êµ¬ì¡° ë³´ì¡´)
    text = pdfplumber_extract(pdf_path)
    is_ok, score = TextQualityChecker().assess(text)
    
    # í’ˆì§ˆ ë¶ˆëŸ‰ ì‹œ OCRë¡œ í´ë°±
    if not is_ok:
        text = paddleocr_extract(pdf_path)
        return text, 'ocr'
        
    return text, 'pdfplumber'
```

### 2.2 ì§€ëŠ¥í˜• ìš”ì•½ í˜ì´ì§€ íƒì§€ (Intelligent Detection)
ëª¨ë“  ë¦¬í¬íŠ¸ê°€ ì²« í˜ì´ì§€ì— ìš”ì•½ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ íƒì§€ ë¡œì§ì„ ì ìš©í•˜ì—¬ íƒìƒ‰ ë²”ìœ„ë¥¼ **ìµœëŒ€ 5í˜ì´ì§€**ë¡œ í™•ì¥í•˜ê³  ë ˆì´ì•„ì›ƒ íŒ¨í„´ì„ ê³ ë ¤í•©ë‹ˆë‹¤.

```python
def detect_summary_page(pages_text: list[str]) -> int:
    """
    í‚¤ì›Œë“œ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ í˜ì´ì§€ë¥¼ ìš”ì•½ í˜ì´ì§€ë¡œ ê°„ì£¼.
    """
    KEYWORDS = ["íˆ¬ìì˜ê²¬", "ëª©í‘œì£¼ê°€", "Buy", "Hold", "TP", "Rating", 
                "EPS", "PER", "í˜„ì¬ì£¼ê°€", "ìƒìŠ¹ì—¬ë ¥"]
    
    best_score, best_idx = -1, 0
    search_range = min(5, len(pages_text)) # íš¨ìœ¨ì„±ì„ ìœ„í•´ ì²˜ìŒ 5í˜ì´ì§€ë§Œ íƒìƒ‰
    
    for i, text in enumerate(pages_text[:search_range]): 
        score = sum(1 for kw in KEYWORDS if kw in text)
        
        # í‘œ êµ¬ì¡°ê°€ ê°ì§€ë˜ë©´ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        if has_table_structure(text): score += 2 
        
        # í‘œì§€ ë‹¤ìŒ í˜ì´ì§€(ì¸ë±ìŠ¤ 1) ìš°ì„ ë„ ì†Œí­ ë¶€ì—¬ (ì¼ë°˜ì ì¸ ë¦¬í¬íŠ¸ êµ¬ì¡°)
        if i == 1: score += 0.5
        
        if score > best_score:
            best_score, best_idx = score, i
            
    return best_idx
```

## 3. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ (Database Schema)

ë°ì´í„° ì •í•©ì„±, í™•ì¥ì„±, ê·¸ë¦¬ê³  pgvectorë¥¼ í™œìš©í•œ ì‹¬ì¸µ ë¶„ì„ì„ ì§€ì›í•˜ëŠ” ìµœì¢… ìŠ¤í‚¤ë§ˆì…ë‹ˆë‹¤.

### 3.1 ERD (Entity Relationship Diagram)

```mermaid
erDiagram
    PERSONS ||--o{ ANALYST_HISTORIES : "has_career"
    PERSONS ||--o{ REPORT_ANALYSTS : "authors"
    PERSONS ||--o{ PREDICTION_OUTCOMES : "predicted"
    
    REPORTS ||--o{ REPORT_ANALYSTS : "written_by"
    REPORTS ||--|| REPORT_FILES : "has_file"
    REPORTS ||--o{ REPORT_OPINIONS : "analyzed_to"
    REPORTS ||--o{ REPORT_EMBEDDINGS : "vectorized_to"
    
    ASSETS_MASTER ||--o{ REPORT_OPINIONS : "target_of"
    ASSETS_MASTER ||--o{ STOCK_PRICES : "has_prices"
    ASSETS_MASTER ||--o{ ASSET_SENTIMENT_DAILY : "has_sentiment"
    ASSETS_MASTER ||--o{ PESSIMISM_PERIODS : "has_periods"

    PERSONS {
        uuid id PK
        string name
        float reliability_30d
        float reliability_365d
        jsonb sector_expertise
    }
    
    REPORT_OPINIONS {
        uuid id PK
        float target_price
        float prev_target_price
        float tp_change_pct
        jsonb key_theses
    }
    
    REPORT_EMBEDDINGS {
        uuid id PK
        vector embedding
        string embedding_type
    }
    
    STOCK_PRICES {
        uuid asset_id FK
        date date
        float close_price
    }
    
    ASSET_SENTIMENT_DAILY {
        uuid asset_id FK
        date date
        float sentiment_score
        int bearish_count
    }
    
    PESSIMISM_PERIODS {
        uuid id PK
        boolean was_actual_bottom
        float return_1y
    }
```

### 3.2 SQL DDL (PostgreSQL)

pgvector í™•ì¥ì„ í¬í•¨í•œ ì „ì²´ í…Œì´ë¸” ì •ì˜ì…ë‹ˆë‹¤.

```sql
-- ==========================================
-- 0. í™•ì¥ ê¸°ëŠ¥ ë° ENUM (Types)
-- ==========================================
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";
CREATE EXTENSION IF NOT EXISTS "pg_trgm"; -- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ìƒ‰ìš©

-- ì˜ê²¬ ìœ í˜•
CREATE TYPE opinion_type AS ENUM ('STRONG_BUY', 'BUY', 'HOLD', 'SELL', 'STRONG_SELL');
-- ì´ì „ ëª©í‘œê°€ ì¶œì²˜
CREATE TYPE prev_tp_source AS ENUM ('db_lookup', 'llm_extracted', 'unavailable');

-- ì²˜ë¦¬ ìƒíƒœ (ë²„ê·¸ ìˆ˜ì •: validation_failed, skipped ì¶”ê°€)
CREATE TYPE processing_status AS ENUM (
    'collected', 'downloaded', 'text_extracted', 
    'llm_processed', 'completed', 
    'failed_download', 'failed_extraction', 'failed_llm',
    'validation_failed', 'skipped'
);

-- ==========================================
-- 1. ë§ˆìŠ¤í„° í…Œì´ë¸” (Master Tables)
-- ==========================================

-- ì¢…ëª© (Asset)
CREATE TABLE assets_master (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    ticker VARCHAR(20) UNIQUE,
    name VARCHAR(100) NOT NULL,
    market VARCHAR(20),
    sector VARCHAR(100),
    aliases JSONB DEFAULT '[]', -- ë³„ì¹­ ê´€ë¦¬ (ì˜ˆ: "ì‚¼ì „" -> "ì‚¼ì„±ì „ì")
    created_at TIMESTAMP DEFAULT NOW()
);
-- ë³„ì¹­ ê²€ìƒ‰ ìµœì í™”
CREATE INDEX idx_assets_aliases ON assets_master USING gin(aliases);

-- ì¦ê¶Œì‚¬
CREATE TABLE securities_firms (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(50) UNIQUE NOT NULL
);

-- ë¦¬í¬íŠ¸ ì¹´í…Œê³ ë¦¬ (ENUM ëŒ€ì‹  í…Œì´ë¸”ë¡œ ìœ ì—°ì„± í™•ë³´)
CREATE TABLE report_categories (
    code VARCHAR(30) PRIMARY KEY, -- 'STOCK_ANALYSIS', 'INDUSTRY_ANALYSIS'
    naver_code VARCHAR(30),
    label_ko VARCHAR(50),
    llm_prompt_template TEXT, -- ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
    is_active BOOLEAN DEFAULT TRUE
);

-- ì• ë„ë¦¬ìŠ¤íŠ¸ (Person) - ë‹¤ì°¨ì› ì‹ ë¢°ë„ ë°˜ì˜
CREATE TABLE persons (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(50) NOT NULL,
    current_firm_id UUID REFERENCES securities_firms(id), -- ìºì‹±ìš© (ë°°ì¹˜ ê°±ì‹ )
    
    -- ë‹¤ì¤‘ Horizon ì‹ ë¢°ë„ (ë‹¨ê¸°/ì¥ê¸° ì „ë¬¸ì„± ë¶„ë¦¬)
    reliability_30d FLOAT DEFAULT 50.0,
    reliability_90d FLOAT DEFAULT 50.0,
    reliability_180d FLOAT DEFAULT 50.0,
    reliability_365d FLOAT DEFAULT 50.0,
    
    sample_count INTEGER DEFAULT 0, -- í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ìš©
    sector_expertise JSONB DEFAULT '{}', -- {"ë°˜ë„ì²´": 85.0, "ë°”ì´ì˜¤": 42.0}
    
    created_at TIMESTAMP DEFAULT NOW()
);

-- ì• ë„ë¦¬ìŠ¤íŠ¸ ì´ë ¥ (ì´ì§ ë° ì‹œê°„ ì—­ìˆœ ì²˜ë¦¬ ëŒ€ì‘)
CREATE TABLE analyst_histories (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    person_id UUID REFERENCES persons(id),
    firm_id UUID REFERENCES securities_firms(id),
    sector VARCHAR(100),
    
    -- í•µì‹¬: Valid Period ê´€ë¦¬ (ì‹œê°„ ì—­ìˆœ ìˆ˜ì§‘ ëŒ€ì‘)
    valid_from DATE NOT NULL, -- í•´ë‹¹ ì†Œì† ì‹œì‘ì¼
    valid_to DATE,            -- í•´ë‹¹ ì†Œì† ì¢…ë£Œì¼ (NULLì´ë©´ í˜„ì¬)
    
    created_at TIMESTAMP DEFAULT NOW()
);
-- ì´ë ¥ ë²”ìœ„ ì¿¼ë¦¬ ìµœì í™”
CREATE INDEX idx_analyst_histories_range 
    ON analyst_histories(person_id, valid_from, valid_to);

-- ==========================================
-- 2. ë¦¬í¬íŠ¸ ë° ì›ë³¸ ê´€ë¦¬ (Report & Files)
-- ==========================================

CREATE TABLE reports (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    title TEXT NOT NULL,
    published_at DATE NOT NULL,
    category_code VARCHAR(30) REFERENCES report_categories(code),
    
    -- ì†ŒìŠ¤ ì¶”ì 
    source_url TEXT,
    naver_report_id VARCHAR(50) UNIQUE, -- ë„¤ì´ë²„ ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€ìš© ID
    source_type VARCHAR(20) DEFAULT 'naver', -- 'naver', 'telegram'
    
    -- ì²˜ë¦¬ ìƒíƒœ
    status processing_status DEFAULT 'collected',
    failed_stage VARCHAR(30),
    retry_count INTEGER DEFAULT 0,
    next_retry_at TIMESTAMP,
    
    created_at TIMESTAMP DEFAULT NOW()
);
-- ë¦¬í¬íŠ¸ ë‚ ì§œ ê¸°ë°˜ ì¡°íšŒ
CREATE INDEX idx_reports_published_at ON reports(published_at DESC);

-- PDF íŒŒì¼ ë° ì¶”ì¶œë¬¼ (ìŠ¤í† ë¦¬ì§€ ìµœì í™”)
CREATE TABLE report_files (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    report_id UUID REFERENCES reports(id),
    file_hash VARCHAR(64) UNIQUE, -- ì¤‘ë³µ ë°©ì§€ (SHA-256)
    storage_path TEXT,            -- PDF íŒŒì¼ ê²½ë¡œ (S3/Local)
    raw_text_path TEXT,           -- ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ ê²½ë¡œ (DB ë¶€í•˜ ë°©ì§€)
    summary_text TEXT,            -- íƒì§€ëœ ìš”ì•½ í˜ì´ì§€ í…ìŠ¤íŠ¸ (ë¶„ì„ìš©)
    extraction_method VARCHAR(20),-- 'pdfplumber', 'ocr'
    created_at TIMESTAMP DEFAULT NOW()
);

-- ë¦¬í¬íŠ¸-ì• ë„ë¦¬ìŠ¤íŠ¸ ë§¤í•‘ (ë‹¤ì¤‘ ì‘ì„±ì ëŒ€ì‘ M:N)
CREATE TABLE report_analysts (
    report_id UUID REFERENCES reports(id),
    person_id UUID REFERENCES persons(id),
    role VARCHAR(20) DEFAULT 'author', -- 'author', 'co_author'
    PRIMARY KEY (report_id, person_id)
);

-- ==========================================
-- 3. ë¶„ì„ ê²°ê³¼ (Analysis Results)
-- ==========================================

-- ê³µí†µ ë¶„ì„ ê²°ê³¼
CREATE TABLE report_opinions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    report_id UUID REFERENCES reports(id),
    asset_id UUID REFERENCES assets_master(id), -- ì¢…ëª© ë¶„ì„ ì‹œì—ë§Œ ìœ íš¨
    
    -- ëª©í‘œê°€ ë° ì˜ê²¬
    target_price FLOAT,
    prev_target_price FLOAT,       -- ì´ì „ ë¦¬í¬íŠ¸ ëŒ€ë¹„
    prev_tp_source prev_tp_source, -- ì¶œì²˜ ëª…ì‹œ ('db_lookup' ìš°ì„ )
    
    current_stock_price FLOAT,     -- ë°œí–‰ì¼ ì¢…ê°€
    opinion_label opinion_type,
    
    -- ê³„ì‚° í•„ë“œ (Generated Columns)
    tp_change_pct FLOAT GENERATED ALWAYS AS (
        CASE 
            WHEN prev_target_price IS NULL OR prev_target_price = 0 THEN NULL
            ELSE ((target_price - prev_target_price) / prev_target_price) * 100 
        END
    ) STORED,
    
    tp_upside_pct FLOAT GENERATED ALWAYS AS (
        CASE 
            WHEN current_stock_price IS NULL OR current_stock_price = 0 THEN NULL
            ELSE ((target_price - current_stock_price) / current_stock_price) * 100 
        END
    ) STORED,
    
    -- LLM ì¶”ì¶œ ë‚´ìš©
    key_facts JSONB,    -- ê°ê´€ì  íŒ©íŠ¸
    key_theses JSONB,   -- ì£¼ê´€ì  ë…¼ë¦¬
    risk_factors JSONB, -- ë¦¬ìŠ¤í¬ ìš”ì¸
    
    -- LLM ë©”íƒ€ë°ì´í„° (ë²„ì „ ê´€ë¦¬)
    llm_raw_response JSONB,    -- ì›ë³¸ ë³´ì¡´ (ì¬ì²˜ë¦¬ìš©)
    llm_model VARCHAR(50),     -- 'gemini-2.0-flash', 'claude-3.7-sonnet'
    llm_prompt_version VARCHAR(20), -- 'v2.1'
    extraction_confidence FLOAT,    -- LLM ìì‹ ê° ì ìˆ˜
    
    created_at TIMESTAMP DEFAULT NOW()
);
-- ìì‚°ë³„ ì˜ê²¬ ì´ë ¥ ì¡°íšŒ
CREATE INDEX idx_report_opinions_asset_date 
    ON report_opinions(asset_id, created_at DESC);
-- í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ ì¬ì²˜ë¦¬ìš©
CREATE INDEX idx_opinions_prompt_version 
    ON report_opinions(llm_prompt_version, llm_model);

-- ì„¹í„° ë¶„ì„ ìƒì„¸ (í™•ì¥ì„±)
CREATE TABLE sector_analysis_details (
    opinion_id UUID PRIMARY KEY REFERENCES report_opinions(id),
    sector_name VARCHAR(100),
    market_outlook VARCHAR(20), -- 'OVERWEIGHT', 'NEUTRAL'
    top_picks JSONB             -- [{"ticker": "...", "reason": "..."}]
);

-- ==========================================
-- 4. ì„ë² ë”© ë° ë²¡í„° ê²€ìƒ‰ (pgvector)
-- ==========================================

CREATE TABLE report_embeddings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    report_id UUID REFERENCES reports(id) ON DELETE CASCADE,
    opinion_id UUID REFERENCES report_opinions(id),
    
    embedding_type VARCHAR(30) NOT NULL, -- 'thesis', 'risk', 'summary', 'combined'
    embedding vector(1536),              -- text-embedding-3-small (1536ì°¨ì›)
    
    model_used VARCHAR(50) DEFAULT 'text-embedding-3-small',
    token_count INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);

-- IVFFlat ì¸ë±ìŠ¤ (ë°ì´í„° 10ë§Œê±´ ì´ìƒ ì‹œ HNSWë¡œ êµì²´ ê³ ë ¤)
CREATE INDEX idx_embeddings_thesis 
    ON report_embeddings USING ivfflat (embedding vector_cosine_ops) 
    WITH (lists = 100)
    WHERE embedding_type = 'thesis';

CREATE INDEX idx_embeddings_risk 
    ON report_embeddings USING ivfflat (embedding vector_cosine_ops) 
    WITH (lists = 100)
    WHERE embedding_type = 'risk';

-- ==========================================
-- 5. ì‹œê³„ì—´ ë¶„ì„ ë° í‰ê°€ (Time-Series & Evaluation)
-- ==========================================

-- ì£¼ê°€ ì‹œê³„ì—´ (ì˜ˆì¸¡ í‰ê°€ + íŒ¨í„´ ë¶„ì„ ê¸°ë°˜)
CREATE TABLE stock_prices (
    asset_id UUID REFERENCES assets_master(id),
    date DATE NOT NULL,
    close_price FLOAT NOT NULL,
    volume BIGINT,
    market_cap BIGINT,
    PRIMARY KEY (asset_id, date)
);
CREATE INDEX idx_stock_prices_date ON stock_prices(date DESC);

-- ì¢…ëª©ë³„ ì¼ë³„ ì„¼í‹°ë©˜íŠ¸ ì§‘ê³„
CREATE TABLE asset_sentiment_daily (
    asset_id UUID REFERENCES assets_master(id),
    date DATE NOT NULL,
    
    bullish_count INTEGER DEFAULT 0,   -- BUY + STRONG_BUY
    neutral_count INTEGER DEFAULT 0,   -- HOLD
    bearish_count INTEGER DEFAULT 0,   -- SELL + STRONG_SELL
    
    avg_target_price FLOAT,
    avg_tp_change_pct FLOAT,           -- í‰ê·  ëª©í‘œê°€ ë³€í™”ìœ¨
    consecutive_tp_down INTEGER,       -- ì—°ì† ëª©í‘œê°€ í•˜í–¥ ê±´ìˆ˜ (ë¹„ê´€ ì‹¬í™” ì§€í‘œ)
    
    sentiment_score FLOAT,             -- (bullish - bearish) / total
    analyst_consensus_divergence FLOAT,-- ì˜ê²¬ ë¶„ì‚°ë„
    
    PRIMARY KEY (asset_id, date)
);

-- ë¹„ê´€ êµ­ë©´ ë§ˆí‚¹ (í•™ìŠµ ë°ì´í„°)
CREATE TABLE pessimism_periods (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    asset_id UUID REFERENCES assets_master(id),
    
    start_date DATE NOT NULL,
    end_date DATE,
    
    min_sentiment_score FLOAT,
    avg_tp_change_pct FLOAT,
    report_count INTEGER,
    
    -- ì‚¬í›„ í‰ê°€
    was_actual_bottom BOOLEAN,
    price_at_start FLOAT,
    price_1y_after FLOAT,
    return_1y FLOAT GENERATED ALWAYS AS (
        CASE WHEN price_at_start > 0 
        THEN (price_1y_after - price_at_start) / price_at_start * 100 
        ELSE NULL END
    ) STORED,
    
    created_at TIMESTAMP DEFAULT NOW()
);

-- ì˜ˆì¸¡ ê²°ê³¼ (ë‹¤ì¤‘ Horizon)
CREATE TABLE prediction_outcomes (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    opinion_id UUID REFERENCES report_opinions(id),
    person_id UUID REFERENCES persons(id), -- ë¹„ì •ê·œí™”
    
    prediction_date DATE NOT NULL,
    predicted_target_price FLOAT NOT NULL,
    
    -- Horizonë³„ ì‹¤ì  (ê°œë³„ ì»¬ëŸ¼ ê´€ë¦¬)
    actual_price_30d FLOAT, is_correct_30d BOOLEAN, evaluated_30d_at TIMESTAMP,
    actual_price_90d FLOAT, is_correct_90d BOOLEAN, evaluated_90d_at TIMESTAMP,
    actual_price_180d FLOAT, is_correct_180d BOOLEAN, evaluated_180d_at TIMESTAMP,
    actual_price_365d FLOAT, is_correct_365d BOOLEAN, evaluated_365d_at TIMESTAMP,
    
    created_at TIMESTAMP DEFAULT NOW()
);
-- í‰ê°€ ëŒ€ê¸° ì¤‘ì¸ ì˜ˆì¸¡ ë°°ì¹˜ìš©
CREATE INDEX idx_prediction_unevaluated 
    ON prediction_outcomes(prediction_date) 
    WHERE evaluated_365d_at IS NULL;
```

## 4. í•µì‹¬ ë¡œì§ (Core Logic)

ë°ì´í„°ì˜ ì •í•©ì„±ì„ ë³´ì¥í•˜ê³  ìš´ì˜ íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•œ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

### 4.1 ëª©í‘œê°€ ì¶œì²˜ í•´ê²° (Target Price Resolver)

ë¦¬í¬íŠ¸ ë³¸ë¬¸ì— ëª…ì‹œëœ "ê¸°ì¡´ ëª©í‘œê°€"ëŠ” ìˆ˜ì • ê³¼ì •ì—ì„œì˜ ì˜¤ë¥˜ë‚˜ ì˜¤íƒ€ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë¯€ë¡œ, **DBì— ì €ì¥ëœ ì§ì „ ë¦¬í¬íŠ¸ì˜ ëª©í‘œê°€**ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì‹ ë¢°í•˜ì—¬ ë°ì´í„° ì •í•©ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

```python
class TargetPriceResolver:
    """
    prev_target_price ê²°ì • ìš°ì„ ìˆœìœ„:
    1. DBì—ì„œ ë™ì¼ ìì‚°ì˜ ì§ì „ ë¦¬í¬íŠ¸ ì¡°íšŒ (ê°€ì¥ ì‹ ë¢°)
    2. LLMì´ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œí•œ ê°’ (ë³´ì¡°)
    3. NULL (ì‚°ì¶œ ë¶ˆê°€)
    """
    async def resolve(
        self, 
        asset_id: UUID, 
        report_date: date,
        llm_extracted_prev_tp: float | None
    ) -> tuple[float | None, str]:
        
        # 1. DBì—ì„œ ë™ì¼ ìì‚°ì˜ ì§ì „ ë¦¬í¬íŠ¸ ì¡°íšŒ (ìµœìš°ì„ )
        db_prev = await self.db.fetchrow("""
            SELECT ro.target_price
            FROM report_opinions ro
            JOIN reports r ON r.id = ro.report_id
            WHERE ro.asset_id = $1 
              AND r.published_at < $2
              AND ro.target_price IS NOT NULL
            ORDER BY r.published_at DESC
            LIMIT 1
        """, asset_id, report_date)
        
        if db_prev:
            return db_prev['target_price'], 'db_lookup'
        
        # 2. LLMì´ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œí•œ ê°’ (ë³´ì¡°)
        if llm_extracted_prev_tp:
            return llm_extracted_prev_tp, 'llm_extracted'
            
        return None, 'unavailable'
```

### 4.2 ì‹œê°„ ì—­ìˆœ ì´ë ¥ ì²˜ë¦¬ (Time-Resistant History)

ë¦¬í¬íŠ¸ê°€ ë°œí–‰ì¼ ì—­ìˆœìœ¼ë¡œ ìˆ˜ì§‘ë˜ë”ë¼ë„ ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì†Œì† ì´ë ¥ì´ ê¼¬ì´ì§€ ì•Šë„ë¡, **Valid Period(ìœ íš¨ê¸°ê°„)** ê°œë…ì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜/ê³¼ê±° ì´ë ¥ê³¼ì˜ ì¶©ëŒì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
async def upsert_analyst_history(
    person_id: UUID, 
    firm_id: UUID, 
    report_date: date
):
    """
    ê³¼ê±° ë¦¬í¬íŠ¸ê°€ ë‚˜ì¤‘ì— ìˆ˜ì§‘ë˜ì–´ë„ ë¯¸ë˜ì˜ ì´ë ¥ ë°ì´í„°ì™€ ì¶©ëŒí•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬.
    """
    async with db.transaction():
        # í•´ë‹¹ ë‚ ì§œ ì´í›„ì˜ ê°€ì¥ ì˜¤ë˜ëœ ì´ë ¥ (Future)
        future_h = await db.fetchrow("""
            SELECT id, firm_id, valid_from FROM analyst_histories
            WHERE person_id = $1 AND valid_from > $2
            ORDER BY valid_from ASC LIMIT 1
        """, person_id, report_date)
        
        # í•´ë‹¹ ë‚ ì§œ ì´ì „ì˜ ê°€ì¥ ìµœê·¼ ì´ë ¥ (Past)
        past_h = await db.fetchrow("""
            SELECT id, firm_id FROM analyst_histories
            WHERE person_id = $1 AND valid_from <= $2
            ORDER BY valid_from DESC LIMIT 1
        """, person_id, report_date)
        
        # ì¼€ì´ìŠ¤ 1: ì´ë¯¸ ë™ì¼ ê¸°ê°„/ì†Œì† ì´ë ¥ì´ ì¡´ì¬í•˜ë©´ ì¢…ë£Œ
        if past_h and past_h['firm_id'] == firm_id:
            return

        # ì¼€ì´ìŠ¤ 2: ë¯¸ë˜ ì´ë ¥ê³¼ ê°™ì€ íšŒì‚¬ë©´ valid_fromì„ ê³¼ê±°ë¡œ ë‹¹ê¹€ (ê¸°ê°„ ì—°ì¥)
        if future_h and future_h['firm_id'] == firm_id:
            await db.execute(
                "UPDATE analyst_histories SET valid_from = $1 WHERE id = $2",
                report_date, future_h['id']
            )
            return

        # ì¼€ì´ìŠ¤ 3: ì‹ ê·œ ì´ë ¥ ìƒì„±
        # ì¢…ë£Œì¼(valid_to)ì€ ë¯¸ë˜ ì´ë ¥ì˜ ì‹œì‘ì¼ í•˜ë£¨ ì „ìœ¼ë¡œ ì„¤ì •
        valid_to = (future_h['valid_from'] - timedelta(days=1)) if future_h else None
        
        await db.execute("""
            INSERT INTO analyst_histories 
            (person_id, firm_id, valid_from, valid_to)
            VALUES ($1, $2, $3, $4)
        """, person_id, firm_id, report_date, valid_to)
```

### 4.3 ì‹ ë¢°ë„ ì ìˆ˜: ì‹œê°„ ê°ì‡  (Time Decay)

ì˜¤ë˜ëœ ì˜ˆì¸¡ë³´ë‹¤ ìµœê·¼ ì˜ˆì¸¡ì˜ ì ì¤‘ ì—¬ë¶€ê°€ ì ìˆ˜ì— ë” í° ì˜í–¥ì„ ì£¼ë„ë¡ ë°˜ì˜í•©ë‹ˆë‹¤. ë˜í•œ ìƒ˜í”Œ ìˆ˜ê°€ ì ì„ ë•Œì˜ í†µê³„ì  í¸í–¥ì„ ë³´ì •í•©ë‹ˆë‹¤.

```python
import math

def calculate_reliability_score(outcomes: list[dict]) -> float:
    """
    ì‹œê°„ ê°ì‡  ê°€ì¤‘ ì‹ ë¢°ë„ ì ìˆ˜
    - ìµœê·¼ ì˜ˆì¸¡ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ë†’ìŒ (ë°˜ê°ê¸°: 365ì¼)
    - ìƒ˜í”Œ ìˆ˜ ë³´ì • ìœ ì§€
    """
    if not outcomes:
        return 50.0
    
    HALF_LIFE_DAYS = 365  # 1ë…„ ì „ ì˜ˆì¸¡ì€ í˜„ì¬ì˜ 50% ê°€ì¤‘ì¹˜
    
    weighted_hits = 0.0
    total_weight = 0.0
    
    for outcome in outcomes:
        days_ago = (date.today() - outcome['prediction_date']).days
        # ì‹œê°„ ê°ì‡  ê°€ì¤‘ì¹˜ ê³„ì‚°
        weight = math.exp(-days_ago * math.log(2) / HALF_LIFE_DAYS)
        
        # 365ì¼ ì ì¤‘ ì—¬ë¶€ ê¸°ì¤€ (ì—†ìœ¼ë©´ 180d, 90d ìˆœ)
        hit = 1 if outcome.get('is_correct_365d') else 0
        
        weighted_hits += weight * hit
        total_weight += weight
    
    if total_weight == 0:
        return 50.0
    
    weighted_rate = weighted_hits / total_weight
    
    # ìƒ˜í”Œ ìˆ˜ ë³´ì • (ìµœì†Œ 10ê±´ì´ì–´ì•¼ Full Credit)
    confidence = min(1.0, len(outcomes) / 10.0)
    
    # ê¸°ë³¸ ì ìˆ˜ 50ì  ê¸°ì¤€ ê³„ì‚°
    return 50 + (weighted_rate * 100 - 50) * confidence
```

---

## 5. LLM ë¶„ì„ ì „ëµ (LLM Strategy)

### 5.1 ëª¨ë¸ ì—­í•  ë¶„ë¦¬ (Model Routing)

ë¹„ìš© íš¨ìœ¨ì„±ê³¼ ë¶„ì„ í’ˆì§ˆì„ ëª¨ë‘ ì¡ê¸° ìœ„í•´ ëª¨ë¸ ìš©ë„ë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•©ë‹ˆë‹¤.

| ì‘ì—… ìœ í˜• | ì‚¬ìš© ëª¨ë¸ | ì´ìœ  |
| :--- | :--- | :--- |
| **êµ¬ì¡°í™” ì¶”ì¶œ** | **Gemini 2.0 Flash** | ì €ë¹„ìš©, ê³ ì† ì²˜ë¦¬. ëŒ€ëŸ‰ì˜ PDF í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ì í•©. Structured Output ì§€ì›. |
| **ì‹¬ì¸µ ë¶„ì„** | **Claude 3.7 Sonnet** | ê°•ë ¥í•œ ì¶”ë¡  ëŠ¥ë ¥(Extended Thinking). ë³µì¡í•œ íŒ¨í„´ ë¶„ì„ ë° ì‹œì¥ ë¹„êµì— ì í•©. |
| **ì„ë² ë”© ìƒì„±** | **text-embedding-3-small** | ì ì ˆí•œ ì°¨ì› ìˆ˜(1536)ì™€ í•œêµ­ì–´ ì„±ëŠ¥ì˜ ê· í˜•. pgvector í˜¸í™˜. |

### 5.2 LLM Router êµ¬í˜„

```python
from anthropic import Anthropic
import google.generativeai as genai
from openai import OpenAI
from pydantic import BaseModel

class StockAnalysisModel(BaseModel):
    """ë‹¨ì¼ ì¢…ëª© ë¶„ì„ ê²°ê³¼ êµ¬ì¡° (Structured Outputìš©)"""
    asset_name: str
    opinion: str
    target_price: float | None
    prev_target_price_in_report: float | None # ë¦¬í¬íŠ¸ì— ëª…ì‹œëœ ê¸°ì¡´ ëª©í‘œê°€
    key_theses: list[str]
    risk_factors: list[str]

class ImprovedLLMRouter:
    
    def __init__(self):
        self.gemini = genai.GenerativeModel("gemini-2.0-flash")
        self.claude = Anthropic()
        self.openai = OpenAI()
        
    async def extract_structured(self, summary_text: str) -> StockAnalysisModel:
        """
        [Gemini Flash] ì €ë¹„ìš© êµ¬ì¡°í™” ì¶”ì¶œ
        ëª©í‘œê°€, íˆ¬ìì˜ê²¬, í•µì‹¬ ë…¼ê±°(Thesis) ì¶”ì¶œ
        """
        response = await self.gemini.generate_content_async(
            contents=f"ë‹¤ìŒ ë¦¬í¬íŠ¸ì—ì„œ ëª©í‘œê°€, íˆ¬ìì˜ê²¬, í•µì‹¬ ë…¼ê±°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:\n{summary_text}",
            generation_config=genai.GenerationConfig(
                response_mime_type="application/json",
                response_schema=StockAnalysisModel,
                temperature=0.0 # ì‚¬ì‹¤ ì¶”ì¶œì´ë¯€ë¡œ ì°½ì˜ì„± 0
            )
        )
        return StockAnalysisModel.model_validate_json(response.text)
    
    async def create_embedding(self, text: str) -> list[float]:
        """
        [text-embedding-3-small] ë²¡í„°í™”
        """
        response = await self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text[:8000] # í† í° ì œí•œ
        )
        return response.data[0].embedding

    async def deep_analyze(
        self, 
        current_context: dict,
        similar_historical: list[dict],
        task: str
    ) -> str:
        """
        [Claude 3.7 Sonnet] Extended Thinking ë”¥ ë¦¬ì„œì¹˜
        """
        prompt = self._build_deep_research_prompt(current_context, similar_historical, task)
        
        response = await self.claude.messages.create(
            model="claude-3-7-sonnet-20250219", # ìµœì‹  ëª¨ë¸
            max_tokens=16000,
            thinking={
                "type": "enabled",
                "budget_tokens": 10000 # ì¶”ë¡  ê¹Šì´ ì„¤ì •
            },
            messages=[{"role": "user", "content": prompt}]
        )
        
        # thinking ë¸”ë¡ê³¼ text ë¸”ë¡ ë¶„ë¦¬ ë°˜í™˜
        thinking_text = next((b.thinking for b in response.content if b.type == "thinking"), "")
        answer_text = next((b.text for b in response.content if b.type == "text"), "")
        
        return {"thinking": thinking_text, "answer": answer_text}

    def _build_deep_research_prompt(self, current, historical, task):
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë¡œì§... (ì´ì „ ë‹µë³€ ì°¸ì¡°)
        pass
```

## 6. ë”¥ ë¦¬ì„œì¹˜ ì—”ì§„ (Deep Research Engine)

"ë¹„ê´€ì˜ ë" íŒ¨í„´ê³¼ ê°™ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œëŠ” ì°¾ê¸° í˜ë“  ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ê³¼ê±° ì‚¬ë¡€ë¥¼ ë¶„ì„í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### 6.1 ë¶„ì„ í”„ë¡œì„¸ìŠ¤

```mermaid
sequenceDiagram
    participant User as ì‚¬ìš©ì
    participant Engine as DeepResearchEngine
    participant DB as PostgreSQL+pgvector
    participant LLM as Claude Sonnet

    User->>Engine: ì¢…ëª© ë¶„ì„ ìš”ì²­ (Asset ID)
    
    Engine->>DB: ìµœê·¼ ë¦¬í¬íŠ¸/ì„¼í‹°ë©˜íŠ¸ ì¡°íšŒ
    DB-->>Engine: í˜„ì¬ Context ë°ì´í„°
    
    Engine->>LLM: í˜„ì¬ Context ì„ë² ë”© ìƒì„± (text-embedding-3-small)
    LLM-->>Engine: Current_Vector (1536ì°¨ì›)
    
    Engine->>DB: pgvector ìœ ì‚¬ë„ ê²€ìƒ‰<br/>(ê³¼ê±° ì €ì  íŒ¨í„´ ì°¾ê¸°)
    Note right of DB: cosine_similarity > 0.75<br/>AND was_actual_bottom = TRUE
    DB-->>Engine: ìœ ì‚¬í•œ ê³¼ì „ ì‚¬ë¡€ ë¦¬ìŠ¤íŠ¸
    
    Engine->>LLM: Claude Extended Thinking<br/>(í˜„ì¬ vs ê³¼ê±° ë¹„êµ ë¶„ì„)
    LLM-->>Engine: ì‹¬ì¸µ ì¸ì‚¬ì´íŠ¸ (Thinking + Answer)
    
    Engine-->>User: ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸
```

### 6.2 êµ¬í˜„ ì½”ë“œ

```python
class DeepResearchEngine:
    """ë¹„ê´€ì˜ ë íŒ¨í„´ ë¶„ì„ ë° ì‹œì¥ ë¹„êµ ì—”ì§„"""
    
    def __init__(self, db, llm_router: ImprovedLLMRouter):
        self.db = db
        self.llm = llm_router

    async def analyze_pessimism_pattern(self, asset_id: UUID) -> dict:
        
        # Step 1: í˜„ì¬ ìƒí™© ì„ë² ë”© ìƒì„±
        current_context = await self._get_current_context(asset_id)
        
        # í•µì‹¬ íˆ¬ì ë…¼ê±°(Theses)ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©í•˜ì—¬ ì„ë² ë”©
        combined_theses = " ".join(current_context["theses"])
        current_embedding = await self.llm.create_embedding(combined_theses)
        
        # Step 2: pgvectorë¡œ ìœ ì‚¬í•œ ê³¼ê±° ë¹„ê´€ êµ­ë©´ ê²€ìƒ‰
        # ì„ë² ë”© ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¿¼ë¦¬
        vector_str = f"[{','.join(map(str, current_embedding))}]"
        
        similar_periods = await self.db.fetch(f"""
            SELECT 
                pp.*,
                asd.sentiment_score,
                asd.consecutive_tp_down,
                sp.close_price as price_at_report_date,
                
                -- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (1 - ê±°ë¦¬)
                1 - (re.embedding <=> '{vector_str}'::vector) AS similarity
            FROM pessimism_periods pp
            JOIN asset_sentiment_daily asd 
                ON asd.asset_id = pp.asset_id AND asd.date = pp.start_date
            JOIN stock_prices sp 
                ON sp.asset_id = pp.asset_id AND sp.date = pp.start_date
            JOIN report_embeddings re ON re.embedding_type = 'thesis'
            JOIN report_opinions ro ON ro.id = re.opinion_id
            JOIN reports r ON r.id = ro.report_id
                AND r.published_at BETWEEN pp.start_date AND COALESCE(pp.end_date, NOW())
            WHERE pp.asset_id = $1
              AND pp.was_actual_bottom IS TRUE  -- ì‚¬í›„ ê²€ì¦ëœ ì €ì ë§Œ ì°¸ì¡°
              AND 1 - (re.embedding <=> '{vector_str}'::vector) > 0.70 -- ìœ ì‚¬ë„ ì„ê³„ê°’
            ORDER BY similarity DESC
            LIMIT 15
        """, asset_id)
        
        # Step 3: Claude Extended Thinkingìœ¼ë¡œ ë”¥ ë¶„ì„
        insight = await self.llm.deep_analyze(
            current_context=current_context,
            similar_historical=similar_periods,
            task_description=f"""
            {current_context['asset_name']}ì˜ í˜„ì¬ ì• ë„ë¦¬ìŠ¤íŠ¸ ë¹„ê´€ êµ­ë©´ì„ 
            ê³¼ê±° ì‹¤ì œ ì €ì ì´ì—ˆë˜ ìœ ì‚¬ íŒ¨í„´ë“¤ê³¼ ë¹„êµí•˜ì—¬:
            1. í˜„ì¬ê°€ ì‹¤ì œ ì €ì ì¼ ê°€ëŠ¥ì„± (ê°€ê²©, ì„¼í‹°ë©˜íŠ¸, í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ê·¼ê±°)
            2. ë°˜ë“± ì‹œ í™•ì¸í•´ì•¼ í•  ì„ í–‰ ì§€í‘œ
            3. ê³¼ê±° ì €ì  êµ­ë©´ê³¼ í˜„ì¬ì˜ ê²°ì •ì  ì°¨ì´ì  (ë¦¬ìŠ¤í¬ ìš”ì¸)
            ë¥¼ ë¶„ì„í•˜ë¼.
            """
        )
        
        return {
            "current_context": current_context,
            "similar_historical_periods": similar_periods,
            "claude_insight": insight,
        }

    async def _get_current_context(self, asset_id: UUID) -> dict:
        """ìµœê·¼ 3ê°œì›” ë¦¬í¬íŠ¸ ê¸°ë°˜ í˜„ì¬ ìƒí™© ìš”ì•½"""
        recent_opinions = await self.db.fetch("""
            SELECT ro.key_theses, ro.risk_factors, ro.target_price, r.published_at
            FROM report_opinions ro
            JOIN reports r ON r.id = ro.report_id
            WHERE ro.asset_id = $1
              AND r.published_at >= NOW() - INTERVAL '90 days'
            ORDER BY r.published_at DESC
        """, asset_id)
        
        sentiment = await self.db.fetchrow("""
            SELECT * FROM asset_sentiment_daily
            WHERE asset_id = $1
            ORDER BY date DESC LIMIT 1
        """, asset_id)
        
        return {
            "asset_id": str(asset_id),
            "recent_opinions": [dict(r) for r in recent_opinions],
            "current_sentiment": dict(sentiment) if sentiment else {},
            "theses": [op["key_theses"] for op in recent_opinions if op.get("key_theses")],
        }
```

---

## 7. ìš´ì˜ ë° ë°°ì¹˜ ì‘ì—… (Operations)

ì‹œìŠ¤í…œì˜ ë°ì´í„° ìµœì‹ ì„±ê³¼ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ì •ê¸° ì‘ì—… ìŠ¤ì¼€ì¤„ì…ë‹ˆë‹¤.

| ì‘ì—… ëª…ì¹­ | ì£¼ê¸° | ì„¤ëª… | ëŒ€ìƒ í…Œì´ë¸” |
| :--- | :--- | :--- | :--- |
| **ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘** | **Daily (ì¥ ë§ˆê° í›„)** | ì˜ˆì¸¡ í‰ê°€ ë° ìˆ˜ìµë¥  ê³„ì‚°ì„ ìœ„í•œ ê¸°ë°˜ ë°ì´í„° í™•ë³´ | `stock_prices` |
| **ì„¼í‹°ë©˜íŠ¸ ì§‘ê³„** | **Daily** | ì¢…ëª©ë³„ ì¼ì¼ ì• ë„ë¦¬ìŠ¤íŠ¸ ì˜ê²¬ ì§‘ê³„ ë° ë¹„ê´€ ì§€í‘œ ê³„ì‚° | `asset_sentiment_daily` |
| **ì„ë² ë”© ìƒì„±** | **ì‹¤ì‹œê°„ (ìˆ˜ì§‘ ì§í›„)** | ì‹ ê·œ ë¦¬í¬íŠ¸ ë¶„ì„ ì™„ë£Œ í›„ ì¦‰ì‹œ ë²¡í„°í™”í•˜ì—¬ ì €ì¥ | `report_embeddings` |
| **ë¹„ê´€ êµ­ë©´ ë§ˆí‚¹** | **Weekly** | ì—°ì†ì ì¸ ëª©í‘œê°€ í•˜í–¥ ë“± ì¡°ê±´ ì¶©ì¡± ì‹œ ë¹„ê´€ êµ­ë©´ìœ¼ë¡œ ë§ˆí‚¹ | `pessimism_periods` |
| **ì‚¬í›„ ì €ì  ê²€ì¦** | **Monthly** | ì§€ë‚œ ë¹„ê´€ êµ­ë©´ ì´í›„ ì£¼ê°€ íë¦„ì„ í™•ì¸í•˜ì—¬ ì‹¤ì œ ì €ì  ì—¬ë¶€ íŒë‹¨ | `pessimism_periods` |
| **ì• ë„ë¦¬ìŠ¤íŠ¸ ì ìˆ˜ ê°±ì‹ ** | **Weekly** | ìƒˆë¡œìš´ í‰ê°€ ê²°ê³¼ ë°˜ì˜ ë° ì‹œê°„ ê°ì‡  ê°€ì¤‘ì¹˜ ì ìš© ì ìˆ˜ ì¬ê³„ì‚° | `persons` |

### 7.1 ë°°ì¹˜ ì‘ì—… êµ¬í˜„ ì˜ˆì‹œ (ì„¼í‹°ë©˜íŠ¸ ì§‘ê³„)

```python
async def aggregate_daily_sentiment(asset_id: UUID, date: date):
    """
    íŠ¹ì • ì¢…ëª©ì˜ íŠ¹ì • ë‚ ì§œ ì„¼í‹°ë©˜íŠ¸ ì§‘ê³„
    """
    # í•´ë‹¹ ë‚ ì§œ ë°œí–‰ëœ ë¦¬í¬íŠ¸ë“¤ì˜ ì˜ê²¬ ì¡°íšŒ
    opinions = await db.fetch("""
        SELECT opinion_label, target_price, tp_change_pct
        FROM report_opinions ro
        JOIN reports r ON r.id = ro.report_id
        WHERE ro.asset_id = $1 AND r.published_at = $2
    """, asset_id, date)
    
    if not opinions:
        return

    # ì§‘ê³„ ë¡œì§
    bullish = sum(1 for o in opinions if o['opinion_label'] in ['BUY', 'STRONG_BUY'])
    bearish = sum(1 for o in opinions if o['opinion_label'] in ['SELL', 'STRONG_SELL'])
    neutral = len(opinions) - bullish - bearish
    
    avg_tp_change = sum(o['tp_change_pct'] for o in opinions if o['tp_change_pct']) / len(opinions)
    
    # ì„¼í‹°ë©˜íŠ¸ ì ìˆ˜ ê³„ì‚° (-100 ~ 100)
    total = len(opinions)
    sentiment_score = ((bullish - bearish) / total) * 100 if total > 0 else 0
    
    # ì €ì¥
    await db.execute("""
        INSERT INTO asset_sentiment_daily 
        (asset_id, date, bullish_count, bearish_count, neutral_count, avg_tp_change_pct, sentiment_score)
        VALUES ($1, $2, $3, $4, $5, $6, $7)
        ON CONFLICT (asset_id, date) DO UPDATE SET
            bullish_count = EXCLUDED.bullish_count,
            sentiment_score = EXCLUDED.sentiment_score,
            avg_tp_change_pct = EXCLUDED.avg_tp_change_pct
    """, asset_id, date, bullish, bearish, neutral, avg_tp_change, sentiment_score)
```

---

## 8. ìœ ì¦ˆì¼€ì´ìŠ¤ (Use Cases)

### Case 1. ìŠ¤í…”ìŠ¤ ë§¤ë„ ì‹ í˜¸ íƒì§€
- **ìƒí™©**: A ì• ë„ë¦¬ìŠ¤íŠ¸ê°€ íˆ¬ìì˜ê²¬ì€ 'ë§¤ìˆ˜'ë¥¼ ìœ ì§€í–ˆìœ¼ë‚˜, ëª©í‘œê°€ë¥¼ 10ë§Œì› â†’ 8ë§Œì›ìœ¼ë¡œ ëŒ€í­ í•˜í–¥ ì¡°ì •.
- **ì‹œìŠ¤í…œ ë™ì‘**:
    1. `Target Price Resolver`ê°€ DB ì¡°íšŒë¥¼ í†µí•´ ì´ì „ ëª©í‘œê°€ 10ë§Œì› í™•ì¸.
    2. `tp_change_pct`ë¥¼ -20%ë¡œ ê³„ì‚°.
    3. ì˜ê²¬(Buy)ê³¼ ëª©í‘œê°€ ë³€í™”(Down)ì˜ ê´´ë¦¬ë¥¼ ê°ì§€í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼.
- **ì¸ì‚¬ì´íŠ¸**: ê³µì‹ ì˜ê²¬ì€ ê¸ì •ì ì´ì§€ë§Œ, ì€ë°€í•œ í•˜í–¥ ì¡°ì •ì€ ê°•ë ¥í•œ ë§¤ë„ ì‹ í˜¸ì„ì„ í¬ì°©.

### Case 2. ì´ì§í•œ ì• ë„ë¦¬ìŠ¤íŠ¸ ì¶”ì 
- **ìƒí™©**: ê³¼ê±° Aì‚¬ì— ìˆë˜ 'ê¹€ì² ìˆ˜' ì• ë„ë¦¬ìŠ¤íŠ¸ê°€ Bì‚¬ë¡œ ì´ì§ í›„ ì²« ë¦¬í¬íŠ¸ ë°œí–‰.
- **ì‹œìŠ¤í…œ ë™ì‘**:
    1. `Time-Resistant History` ë¡œì§ì´ ê³¼ê±° ë°ì´í„°ì™€ í˜„ì¬ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì—°ê²°.
    2. ì‹ ë¢°ë„ ì ìˆ˜ëŠ” ì´ë ¥ì´ ë³€ê²½ë˜ì–´ë„ ê°œì¸ì—ê²Œ ê·€ì†ë˜ì–´ ìœ ì§€ë¨.
- **ì¸ì‚¬ì´íŠ¸**: "ì´ì§ í›„ ì²« ë¦¬í¬íŠ¸. ê³¼ê±° ë°ì´í„° ê¸°ë°˜ ì‹ ë¢°ë„(85ì )ê°€ ì—¬ì „íˆ ìœ íš¨í•¨. ìƒˆë¡œìš´ ì†Œì†ì‚¬ì—ì„œì˜ ëª©ì†Œë¦¬ í™•ì¸ í•„ìš”."

### Case 3. "ë¹„ê´€ì˜ ë" ë”¥ ë¦¬ì„œì¹˜ (Deep Research)
- **ìƒí™©**: íŠ¹ì • ì¢…ëª©ì´ ì—°ì†ì ì¸ ëª©í‘œê°€ í•˜í–¥ìœ¼ë¡œ ì‹œì¥ ë¹„ê´€ ì‹¬í™” ì¤‘.
- **ì‹œìŠ¤í…œ ë™ì‘**:
    1. `asset_sentiment_daily` í…Œì´ë¸”ì—ì„œ `consecutive_tp_down` ìƒìŠ¹ ê°ì§€.
    2. ì‚¬ìš©ì ìš”ì²­ ì‹œ `DeepResearchEngine` ì‘ë™.
    3. pgvectorë¡œ ê³¼ê±° ì‹¤ì œ ì €ì ì´ì—ˆë˜ ìœ ì‚¬ ìƒí™©(í…ìŠ¤íŠ¸, ìˆ˜ì¹˜ ìœ ì‚¬ì„±) ê²€ìƒ‰.
    4. Claude 3.7 Sonnetì´ í˜„ì¬ ìƒí™©ê³¼ ê³¼ê±° ì‚¬ë¡€ ë¹„êµ ë¶„ì„.
- **ì¸ì‚¬ì´íŠ¸**: "ê³¼ê±° ë°˜ë„ì²´ ì—…í™© ì•…í™” ì‹œê¸°(2023ë…„ 10ì›”)ì™€ í…ìŠ¤íŠ¸ íŒ¨í„´ì´ 88% ìœ ì‚¬. ë‹¹ì‹œ ì €ì  ì´í›„ 6ê°œì›”ê°„ 45% ìƒìŠ¹. í˜„ì¬ ë¦¬ìŠ¤í¬ëŠ” ë°¸ë¥˜ì—ì´ì…˜ ì €í‰ê°€ë¡œ ìƒì‡„ë  ê°€ëŠ¥ì„± ì œê¸°."

---

## 9. ê²°ë¡  (Conclusion)

ë³¸ ì„¤ê³„ì„œëŠ” ë‹¨ìˆœí•œ ë¦¬í¬íŠ¸ ìš”ì•½ ì‹œìŠ¤í…œì„ ë„˜ì–´, **'ì‹œì¥ì˜ ì™œê³¡ëœ ì‹ í˜¸ë¥¼ êµì •'**í•˜ê³  **'ì—­ì‚¬ì  íŒ¨í„´ ê¸°ë°˜ì˜ ë¯¸ë˜ ì˜ˆì¸¡'**ì„ ì§€ì›í•˜ëŠ” ì¢…í•© íˆ¬ì ì¸í…”ë¦¬ì „ìŠ¤ í”Œë«í¼ì„ ì •ì˜í•©ë‹ˆë‹¤.

### 9.1 ì£¼ìš” ê²½ìŸë ¥
1.  **ë°ì´í„° ì •í•©ì„±**: ì‹œê°„ ì—­ìˆœ ìˆ˜ì§‘, í…ìŠ¤íŠ¸ í’ˆì§ˆ ì €í•˜, ëª©í‘œê°€ ì¶œì²˜ ë¶ˆëª… ë“± ì‹¤ì œ ë°ì´í„° í™˜ê²½ì˜ ë¬¸ì œë¥¼ ê¸°ìˆ ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
2.  **ê²½ì œì  íš¨ìœ¨ì„±**: Gemini Flashì™€ Claude Sonnetì˜ ì—­í•  ë¶„ë¦¬, ìŠ¤í† ë¦¬ì§€ ë¶„ë¦¬ë¥¼ í†µí•´ ìš´ì˜ ë¹„ìš©ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.
3.  **ì‹¬ì¸µ ë¶„ì„ ëŠ¥ë ¥**: pgvectorì™€ Claude Extended Thinkingì„ ê²°í•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒ¨í„´ ì¸ì‹ ë° ì‹¬ì¸µ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.

### 9.2 ê¸°ëŒ€ íš¨ê³¼
ì´ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìì—ê²Œ ë‹¨ìˆœí•œ ì •ë³´ ì œê³µì„ ë„˜ì–´, **"ì–´ë–¤ ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì–´ë–¤ ì‹ í˜¸ë¥¼ ë¯¿ì„ ê²ƒì¸ê°€"**ì— ëŒ€í•œ ê°ê´€ì ì¸ íŒë‹¨ ê·¼ê±°ì™€, **"ê³¼ê±°ì—ëŠ” ì´ëŸ° ìƒí™©ì—ì„œ ì–´ë–»ê²Œ í–ˆëŠ”ê°€"**ë¼ëŠ” ì—­ì‚¬ì  í†µì°°ì„ ì œê³µí•¨ìœ¼ë¡œì¨ íˆ¬ì ì˜ì‚¬ê²°ì •ì˜ ì§ˆì„ íšê¸°ì ìœ¼ë¡œ ë†’ì¼ ê²ƒì…ë‹ˆë‹¤.